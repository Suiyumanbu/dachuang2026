# predict.py

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # <--- æ·»åŠ è¿™ä¸€è¡Œï¼Œåœ¨å¯¼å…¥pyplotä¹‹å‰
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix, classification_report)
from sklearn.preprocessing import StandardScaler
import warnings
from process_data import process_data, read_fasta  # å¯¼å…¥å¤„ç†æ•°æ®çš„å‡½æ•°
import joblib

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams["font.sans-serif"] = ["SimHei"]
plt.rcParams["axes.unicode_minus"] = False

# å®šä¹‰è·¯å¾„
path = Path('D:\Python\dachuang2026')


def prepare_features(df):
    """
    å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆæ ‡ç­¾å·²ç»æ˜¯'0'å’Œ'1'å­—ç¬¦ä¸²ï¼‰
    """
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(['Sequence', 'toxicity'], axis=1)
    y = df['toxicity']

    # å°†å­—ç¬¦ä¸²'0'/'1'è½¬æ¢ä¸ºæ•´æ•°
    y_numeric = y.astype(int)

    # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åˆæ³•
    unique_values = y_numeric.unique()
    if set(unique_values) - {0, 1}:
        print(f"è­¦å‘Š: å‘ç°é0/1çš„æ ‡ç­¾å€¼: {unique_values}")

    print(f"æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"  ç±»åˆ«0 (æ— æ¯’): {(y_numeric == 0).sum()} ä¸ªæ ·æœ¬ ({((y_numeric == 0).sum()/len(y_numeric)*100):.1f}%)")
    print(f"  ç±»åˆ«1 (æœ‰æ¯’): {(y_numeric == 1).sum()} ä¸ªæ ·æœ¬ ({((y_numeric == 1).sum()/len(y_numeric)*100):.1f}%)")

    return X, y_numeric

def train_random_forest(X_train, y_train):
    """
    è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼ˆä¸ä½¿ç”¨äº¤å‰éªŒè¯ï¼‰
    """
    print("\n" + "=" * 50)
    print("å¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    print("=" * 50)

    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    print("\nè®­ç»ƒæ¨¡å‹...")
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    )

    rf_model.fit(X_train_scaled, y_train)

    print("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    return rf_model, scaler


def evaluate_model(model, X_test, y_test):
    """
    è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
    """
    print("\n" + "=" * 50)
    print("æµ‹è¯•é›†è¯„ä¼°ç»“æœ")
    print("=" * 50)

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')  # äºŒåˆ†ç±»ç”¨binary
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')

    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"å¬å›ç‡ (Recall): {recall:.4f}")
    print(f"F1åˆ†æ•° (F1-score): {f1:.4f}")

    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=['æ— æ¯’(0)', 'æœ‰æ¯’(1)']))

    return y_pred, y_pred_proba


def plot_confusion_matrix(y_test, y_pred, save_path):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ
    """
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'],
                yticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'])
    plt.title('æ··æ·†çŸ©é˜µ - æµ‹è¯•é›†')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

    # è®¡ç®—å¹¶æ˜¾ç¤ºé¢å¤–æŒ‡æ ‡
    tn, fp, fn, tp = cm.ravel()
    print(f"\næ··æ·†çŸ©é˜µè¯¦æƒ…:")
    print(f"  çœŸé˜´æ€§ (TN): {tn} (æ­£ç¡®é¢„æµ‹ä¸ºæ— æ¯’)")
    print(f"  å‡é˜³æ€§ (FP): {fp} (æ— æ¯’è¢«è¯¯åˆ¤ä¸ºæœ‰æ¯’)")
    print(f"  å‡é˜´æ€§ (FN): {fn} (æœ‰æ¯’è¢«è¯¯åˆ¤ä¸ºæ— æ¯’)")
    print(f"  çœŸé˜³æ€§ (TP): {tp} (æ­£ç¡®é¢„æµ‹ä¸ºæœ‰æ¯’)")


def plot_feature_importance(model, feature_names, save_path, top_n=20):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾
    """
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    # å–å‰top_nä¸ªç‰¹å¾
    top_indices = indices[:top_n]
    top_importances = importances[top_indices]
    top_names = [feature_names[i] for i in top_indices]

    plt.figure(figsize=(10, 8))
    plt.barh(range(top_n), top_importances[::-1])
    plt.yticks(range(top_n), top_names[::-1])
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'å‰{top_n}ä¸ªæœ€é‡è¦çš„ç‰¹å¾')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {save_path}")

    # æ‰“å°æœ€é‡è¦çš„å‡ ä¸ªç‰¹å¾
    print(f"\nå‰{top_n}ä¸ªæœ€é‡è¦çš„ç‰¹å¾:")
    for i, (name, importance) in enumerate(zip(top_names, top_importances)):
        print(f"{i + 1}. {name}: {importance:.4f}")


def save_predictions(model, X_test, y_test, y_pred, save_path):
    """
    ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœ
    """
    y_pred_proba = model.predict_proba(X_test)

    results = pd.DataFrame({
        'çœŸå®æ ‡ç­¾': y_test,
        'é¢„æµ‹æ ‡ç­¾': y_pred,
        'é¢„æµ‹æ­£ç¡®': y_test == y_pred,
        'æ— æ¯’æ¦‚ç‡': y_pred_proba[:, 0],
        'æœ‰æ¯’æ¦‚ç‡': y_pred_proba[:, 1]
    })

    # æ·»åŠ é¢„æµ‹ç»“æœè§£é‡Š
    results['çœŸå®ç±»åˆ«'] = results['çœŸå®æ ‡ç­¾'].map({0: 'æ— æ¯’', 1: 'æœ‰æ¯’'})
    results['é¢„æµ‹ç±»åˆ«'] = results['é¢„æµ‹æ ‡ç­¾'].map({0: 'æ— æ¯’', 1: 'æœ‰æ¯’'})

    results.to_csv(save_path, index=False)
    print(f"è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {save_path}")

    # æ˜¾ç¤ºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    errors = results[results['é¢„æµ‹æ­£ç¡®'] == False]
    if len(errors) > 0:
        print(f"\né”™è¯¯åˆ†ç±»çš„æ ·æœ¬æ•°: {len(errors)} ({len(errors)/len(y_test)*100:.1f}%)")
        print("\né”™è¯¯åˆ†ç±»ç¤ºä¾‹:")
        print(errors[['çœŸå®ç±»åˆ«', 'é¢„æµ‹ç±»åˆ«', 'æ— æ¯’æ¦‚ç‡', 'æœ‰æ¯’æ¦‚ç‡']].head())


def main():
    """
    ä¸»å‡½æ•°ï¼šä½¿ç”¨train.fastaè®­ç»ƒï¼Œç”¨test.fastaæµ‹è¯•
    """
    print("=" * 60)
    print("è›‹ç™½è´¨æ¯’æ€§é¢„æµ‹ - éšæœºæ£®æ—æ¨¡å‹")
    print("(æ ‡ç­¾æ ¼å¼: 0=æ— æ¯’, 1=æœ‰æ¯’)")
    print("=" * 60)

    # åˆ›å»ºç»“æœç›®å½•
    results_dir = path / 'results'
    results_dir.mkdir(exist_ok=True)

    # 1. åŠ è½½å¹¶å¤„ç†è®­ç»ƒæ•°æ®
    print("\nã€1ã€‘åŠ è½½è®­ç»ƒæ•°æ®...")
    train_file = path / 'data/train_data.fasta'
    if not train_file.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ–‡ä»¶ {train_file}")
        return

    train_data = read_fasta(train_file)
    print(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}")
    processed_train = process_data(train_data)

    # 2. å‡†å¤‡è®­ç»ƒç‰¹å¾å’Œæ ‡ç­¾
    print("\nã€2ã€‘å‡†å¤‡è®­ç»ƒç‰¹å¾...")
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X_train = processed_train.drop(['Sequence', 'toxicity'], axis=1)
    y_train = processed_train['toxicity'].astype(int)  # å­—ç¬¦ä¸²'0'/'1'è½¬æ•´æ•°

    # æ˜¾ç¤ºè®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ
    print(f"  ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
    print(f"  è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"    æ— æ¯’ (0): {(y_train == 0).sum()} æ ·æœ¬ ({((y_train == 0).sum() / len(y_train) * 100):.1f}%)")
    print(f"    æœ‰æ¯’ (1): {(y_train == 1).sum()} æ ·æœ¬ ({((y_train == 1).sum() / len(y_train) * 100):.1f}%)")

    # 3. è®­ç»ƒæ¨¡å‹
    print("\nã€3ã€‘è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    )

    model.fit(X_train_scaled, y_train)
    print("  æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    # 4. åŠ è½½å¹¶å¤„ç†æµ‹è¯•æ•°æ®
    print("\nã€4ã€‘åŠ è½½æµ‹è¯•æ•°æ®...")
    test_file = path / 'data/test1.fasta'
    if not test_file.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æµ‹è¯•æ–‡ä»¶ {test_file}")
        return

    test_data = read_fasta(test_file)
    print(f"  æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)}")
    processed_test = process_data(test_data)

    # 5. å‡†å¤‡æµ‹è¯•ç‰¹å¾å’Œæ ‡ç­¾
    print("\nã€5ã€‘å‡†å¤‡æµ‹è¯•ç‰¹å¾...")
    X_test = processed_test.drop(['Sequence', 'toxicity'], axis=1)
    y_test = processed_test['toxicity'].astype(int)

    # æ˜¾ç¤ºæµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ
    print(f"  æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"    æ— æ¯’ (0): {(y_test == 0).sum()} æ ·æœ¬ ({((y_test == 0).sum() / len(y_test) * 100):.1f}%)")
    print(f"    æœ‰æ¯’ (1): {(y_test == 1).sum()} æ ·æœ¬ ({((y_test == 1).sum() / len(y_test) * 100):.1f}%)")

    # ç¡®ä¿æµ‹è¯•é›†çš„ç‰¹å¾åˆ—ä¸è®­ç»ƒé›†ä¸€è‡´
    missing_cols = set(X_train.columns) - set(X_test.columns)
    for col in missing_cols:
        X_test[col] = 0

    X_test = X_test[X_train.columns]  # ä¿æŒåˆ—é¡ºåºä¸€è‡´

    # æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®
    X_test_scaled = scaler.transform(X_test)

    # 6. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
    print("\nã€6ã€‘åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹...")
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)

    # ã€7ã€‘æ¨¡å‹è¯„ä¼°ç»“æœ
    print("\nã€7ã€‘æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("-" * 40)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # è®¡ç®—é¢„æµ‹æ­£ç¡®ç‡
    correct_predictions = (y_test == y_pred).sum()
    total_predictions = len(y_test)
    correct_rate = correct_predictions / total_predictions

    print(f"  é¢„æµ‹æ­£ç¡®æ•°: {correct_predictions}/{total_predictions}")
    print(f"  é¢„æµ‹æ­£ç¡®ç‡: {correct_rate:.4f} ({correct_rate * 100:.2f}%)")
    print(f"  é¢„æµ‹é”™è¯¯æ•°: {total_predictions - correct_predictions}/{total_predictions}")
    print(f"  é¢„æµ‹é”™è¯¯ç‡: {1 - correct_rate:.4f} ({(1 - correct_rate) * 100:.2f}%)")
    print("-" * 40)
    print(f"  å‡†ç¡®ç‡ (Accuracy):  {accuracy:.4f}")
    print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"  å¬å›ç‡ (Recall):    {recall:.4f}")
    print(f"  F1åˆ†æ•° (F1-score):  {f1:.4f}")

    # 8. æ··æ·†çŸ©é˜µ
    print("\nã€8ã€‘æ··æ·†çŸ©é˜µ")
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    print("-" * 40)
    print(f"              é¢„æµ‹æ— æ¯’    é¢„æµ‹æœ‰æ¯’")
    print(f"  å®é™…æ— æ¯’    {tn:6d}      {fp:6d}")
    print(f"  å®é™…æœ‰æ¯’    {fn:6d}      {tp:6d}")
    print("-" * 40)

    # è®¡ç®—æ›´å¤šæŒ‡æ ‡
    sensitivity = tp / (tp + fn)  # å¬å›ç‡/æ•æ„Ÿåº¦
    specificity = tn / (tn + fp)  # ç‰¹å¼‚æ€§
    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # é˜³æ€§é¢„æµ‹å€¼
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # é˜´æ€§é¢„æµ‹å€¼

    print(f"\n  æ•æ„Ÿåº¦ (Sensitivity): {sensitivity:.4f} (æ­£ç¡®è¯†åˆ«æœ‰æ¯’çš„èƒ½åŠ›)")
    print(f"  ç‰¹å¼‚æ€§ (Specificity): {specificity:.4f} (æ­£ç¡®è¯†åˆ«æ— æ¯’çš„èƒ½åŠ›)")
    print(f"  é˜³æ€§é¢„æµ‹å€¼ (PPV):     {ppv:.4f} (é¢„æµ‹ä¸ºæœ‰æ¯’ä¸­å®é™…æœ‰æ¯’çš„æ¯”ä¾‹)")
    print(f"  é˜´æ€§é¢„æµ‹å€¼ (NPV):     {npv:.4f} (é¢„æµ‹ä¸ºæ— æ¯’ä¸­å®é™…æ— æ¯’çš„æ¯”ä¾‹)")

    # 9. ä¿å­˜å¯è§†åŒ–ç»“æœ
    print("\nã€9ã€‘ä¿å­˜å¯è§†åŒ–ç»“æœ...")

    # ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'],
                yticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'])
    plt.title('æ··æ·†çŸ©é˜µ - æµ‹è¯•é›†')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig(results_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close('all')
    print(f"  æ··æ·†çŸ©é˜µå·²ä¿å­˜: {results_dir / 'confusion_matrix.png'}")

    # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:20]  # å–å‰20ä¸ª

    plt.figure(figsize=(10, 8))
    plt.barh(range(20), importances[indices][::-1])
    plt.yticks(range(20), [X_train.columns[i] for i in indices][::-1])
    plt.xlabel('é‡è¦æ€§')
    plt.title('å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾')
    plt.tight_layout()
    plt.savefig(results_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')
    plt.close('all')
    print(f"  ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {results_dir / 'feature_importance.png'}")

    # 10. ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    print("\nã€10ã€‘ä¿å­˜é¢„æµ‹ç»“æœ...")

    results_df = pd.DataFrame({
        'çœŸå®æ ‡ç­¾': y_test,
        'é¢„æµ‹æ ‡ç­¾': y_pred,
        'é¢„æµ‹æ­£ç¡®': y_test == y_pred,
        'æ— æ¯’æ¦‚ç‡': y_pred_proba[:, 0],
        'æœ‰æ¯’æ¦‚ç‡': y_pred_proba[:, 1]
    })

    # æ·»åŠ å¯è¯»æ€§æ›´å¥½çš„ç±»åˆ«åˆ—
    results_df['çœŸå®ç±»åˆ«'] = results_df['çœŸå®æ ‡ç­¾'].map({0: 'æ— æ¯’', 1: 'æœ‰æ¯’'})
    results_df['é¢„æµ‹ç±»åˆ«'] = results_df['é¢„æµ‹æ ‡ç­¾'].map({0: 'æ— æ¯’', 1: 'æœ‰æ¯’'})

    # æ·»åŠ åŸå§‹åºåˆ—ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    if 'Sequence' in processed_test.columns:
        results_df['åºåˆ—'] = processed_test['Sequence'].values

    results_df.to_csv(results_dir / 'test_predictions.csv', index=False)
    print(f"  é¢„æµ‹ç»“æœå·²ä¿å­˜: {results_dir / 'test_predictions.csv'}")

    # æ˜¾ç¤ºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    errors = results_df[results_df['é¢„æµ‹æ­£ç¡®'] == False]
    if len(errors) > 0:
        print(f"\n  é”™è¯¯åˆ†ç±»æ ·æœ¬æ•°: {len(errors)} ({len(errors) / len(y_test) * 100:.1f}%)")
        print("\n  é”™è¯¯åˆ†ç±»ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
        print(errors[['çœŸå®ç±»åˆ«', 'é¢„æµ‹ç±»åˆ«', 'æ— æ¯’æ¦‚ç‡', 'æœ‰æ¯’æ¦‚ç‡']].head())

    # 11. ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶
    print("\nã€11ã€‘ä¿å­˜æ¨¡å‹...")
    joblib.dump(model, results_dir / 'random_forest_model.pkl')
    joblib.dump(scaler, results_dir / 'scaler.pkl')

    # ä¿å­˜ç‰¹å¾åç§°
    pd.Series(X_train.columns).to_csv(results_dir / 'feature_names.csv', index=False)

    # ä¿å­˜æ¨¡å‹é…ç½®
    model_config = {
        'model_type': 'RandomForestClassifier',
        'n_estimators': 100,
        'max_depth': 10,
        'min_samples_split': 5,
        'min_samples_leaf': 2,
        'class_weight': 'balanced',
        'random_state': 42,
        'n_features': X_train.shape[1],
        'features': list(X_train.columns)
    }

    import json
    with open(results_dir / 'model_config.json', 'w', encoding='utf-8') as f:
        json.dump(model_config, f, indent=2, ensure_ascii=False)

    print(f"  æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜: {results_dir / 'random_forest_model.pkl'}")
    print(f"  æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {results_dir / 'scaler.pkl'}")
    print(f"  ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜: {results_dir / 'feature_names.csv'}")
    print(f"  æ¨¡å‹é…ç½®å·²ä¿å­˜: {results_dir / 'model_config.json'}")

    # 12. è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“Š ç»“æœæ±‡æ€»:")
    print(f"  - è®­ç»ƒé›†å¤§å°: {len(X_train)} ä¸ªæ ·æœ¬")
    print(f"  - æµ‹è¯•é›†å¤§å°: {len(X_test)} ä¸ªæ ·æœ¬")
    print(f"  - ç‰¹å¾æ•°é‡: {X_train.shape[1]}")
    print(f"  - å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"  - F1åˆ†æ•°: {f1:.4f}")
    print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®: {results_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
    plt.close('all')
"""
============================================================
è›‹ç™½è´¨æ¯’æ€§é¢„æµ‹ - éšæœºæ£®æ—æ¨¡å‹
(æ ‡ç­¾æ ¼å¼: 0=æ— æ¯’, 1=æœ‰æ¯’)
============================================================

ã€1ã€‘åŠ è½½è®­ç»ƒæ•°æ®...
  è®­ç»ƒé›†æ ·æœ¬æ•°: 6387

å¤„ç†å®Œæˆï¼
æ ·æœ¬æ•°: 6387
ç‰¹å¾æ•°: 132
æ ·æœ¬/ç‰¹å¾æ¯”: 48.4:1

ã€2ã€‘å‡†å¤‡è®­ç»ƒç‰¹å¾...
  ç‰¹å¾ç»´åº¦: 132
  è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:
    æ— æ¯’ (0): 4569 æ ·æœ¬ (71.5%)
    æœ‰æ¯’ (1): 1818 æ ·æœ¬ (28.5%)

ã€3ã€‘è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...
  æ¨¡å‹è®­ç»ƒå®Œæˆï¼

ã€4ã€‘åŠ è½½æµ‹è¯•æ•°æ®...
  æµ‹è¯•é›†æ ·æœ¬æ•°: 1126

å¤„ç†å®Œæˆï¼
æ ·æœ¬æ•°: 1126
ç‰¹å¾æ•°: 132
æ ·æœ¬/ç‰¹å¾æ¯”: 8.5:1

ã€5ã€‘å‡†å¤‡æµ‹è¯•ç‰¹å¾...
  æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ:
    æ— æ¯’ (0): 806 æ ·æœ¬ (71.6%)
    æœ‰æ¯’ (1): 320 æ ·æœ¬ (28.4%)

ã€6ã€‘åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹...

ã€7ã€‘æ¨¡å‹è¯„ä¼°ç»“æœ
----------------------------------------
  é¢„æµ‹æ­£ç¡®æ•°: 1023/1126
  é¢„æµ‹æ­£ç¡®ç‡: 0.9085 (90.85%)
  é¢„æµ‹é”™è¯¯æ•°: 103/1126
  é¢„æµ‹é”™è¯¯ç‡: 0.0915 (9.15%)
----------------------------------------
  å‡†ç¡®ç‡ (Accuracy):  0.9085
  ç²¾ç¡®ç‡ (Precision): 0.8678
  å¬å›ç‡ (Recall):    0.8000
  F1åˆ†æ•° (F1-score):  0.8325

ã€8ã€‘æ··æ·†çŸ©é˜µ
----------------------------------------
              é¢„æµ‹æ— æ¯’    é¢„æµ‹æœ‰æ¯’
  å®é™…æ— æ¯’       767          39
  å®é™…æœ‰æ¯’        64         256
----------------------------------------

  æ•æ„Ÿåº¦ (Sensitivity): 0.8000 (æ­£ç¡®è¯†åˆ«æœ‰æ¯’çš„èƒ½åŠ›)
  ç‰¹å¼‚æ€§ (Specificity): 0.9516 (æ­£ç¡®è¯†åˆ«æ— æ¯’çš„èƒ½åŠ›)
  é˜³æ€§é¢„æµ‹å€¼ (PPV):     0.8678 (é¢„æµ‹ä¸ºæœ‰æ¯’ä¸­å®é™…æœ‰æ¯’çš„æ¯”ä¾‹)
  é˜´æ€§é¢„æµ‹å€¼ (NPV):     0.9230 (é¢„æµ‹ä¸ºæ— æ¯’ä¸­å®é™…æ— æ¯’çš„æ¯”ä¾‹)

============================================================
âœ… æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼
============================================================

ğŸ“Š ç»“æœæ±‡æ€»:
  - è®­ç»ƒé›†å¤§å°: 6387 ä¸ªæ ·æœ¬
  - æµ‹è¯•é›†å¤§å°: 1126 ä¸ªæ ·æœ¬
  - ç‰¹å¾æ•°é‡: 132
  - å‡†ç¡®ç‡: 0.9085
  - F1åˆ†æ•°: 0.8325
============================================================
"""