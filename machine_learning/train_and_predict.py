"""
éšæœºæ£®æ—æ¯’æ€§é¢„æµ‹æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹è„šæœ¬
ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è›‹ç™½è´¨æ¯’æ€§é¢„æµ‹æ¨¡å‹
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix, classification_report)
from sklearn.preprocessing import StandardScaler
import warnings
from machine_learning.process_data import process_data, read_fasta
import joblib

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams["font.sans-serif"] = ["SimHei"]
plt.rcParams["axes.unicode_minus"] = False


def prepare_features(df):
    """
    å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆæ ‡ç­¾å·²ç»æ˜¯'0'å’Œ'1'å­—ç¬¦ä¸²ï¼‰

    Parameters:
    -----------
    df : pandas.DataFrame
        åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„æ•°æ®æ¡†

    Returns:
    --------
    X : pandas.DataFrame
        ç‰¹å¾æ•°æ®
    y_numeric : pandas.Series
        æ•°å€¼å‹æ ‡ç­¾ï¼ˆ0/1ï¼‰
    """
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(['Sequence', 'toxicity'], axis=1)
    y = df['toxicity']

    # å°†å­—ç¬¦ä¸²'0'/'1'è½¬æ¢ä¸ºæ•´æ•°
    y_numeric = y.astype(int)

    return X, y_numeric


def train_random_forest(X_train, y_train):
    """
    è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼ˆä¸ä½¿ç”¨äº¤å‰éªŒè¯ï¼‰

    Parameters:
    -----------
    X_train : pandas.DataFrame
        è®­ç»ƒç‰¹å¾æ•°æ®
    y_train : pandas.Series
        è®­ç»ƒæ ‡ç­¾æ•°æ®

    Returns:
    --------
    rf_model : RandomForestClassifier
        è®­ç»ƒå¥½çš„éšæœºæ£®æ—æ¨¡å‹
    scaler : StandardScaler
        æ ‡å‡†åŒ–å™¨
    """
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    )

    rf_model.fit(X_train_scaled, y_train)

    return rf_model, scaler


def evaluate_model(model, X_test, y_test):
    """
    è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½

    Parameters:
    -----------
    model : RandomForestClassifier
        è®­ç»ƒå¥½çš„æ¨¡å‹
    X_test : array-like
        æµ‹è¯•ç‰¹å¾æ•°æ®
    y_test : array-like
        æµ‹è¯•æ ‡ç­¾æ•°æ®

    Returns:
    --------
    y_pred : array
        é¢„æµ‹æ ‡ç­¾
    y_pred_proba : array
        é¢„æµ‹æ¦‚ç‡
    """
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')

    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"å¬å›ç‡ (Recall): {recall:.4f}")
    print(f"F1åˆ†æ•° (F1-score): {f1:.4f}")

    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=['æ— æ¯’(0)', 'æœ‰æ¯’(1)']))

    return y_pred, y_pred_proba


def plot_confusion_matrix(y_test, y_pred, save_path):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ

    Parameters:
    -----------
    y_test : array-like
        çœŸå®æ ‡ç­¾
    y_pred : array-like
        é¢„æµ‹æ ‡ç­¾
    save_path : Path or str
        ä¿å­˜è·¯å¾„
    """
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'],
                yticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'])
    plt.title('æ··æ·†çŸ©é˜µ - æµ‹è¯•é›†')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.show()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")


def plot_feature_importance(model, feature_names, save_path, top_n=20):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾

    Parameters:
    -----------
    model : RandomForestClassifier
        è®­ç»ƒå¥½çš„æ¨¡å‹
    feature_names : list
        ç‰¹å¾åç§°åˆ—è¡¨
    save_path : Path or str
        ä¿å­˜è·¯å¾„
    top_n : int
        æ˜¾ç¤ºæœ€é‡è¦çš„å‰Nä¸ªç‰¹å¾
    """
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    # å–å‰top_nä¸ªç‰¹å¾
    top_indices = indices[:top_n]
    top_importances = importances[top_indices]
    top_names = [feature_names[i] for i in top_indices]

    plt.figure(figsize=(10, 8))
    plt.barh(range(top_n), top_importances[::-1])
    plt.yticks(range(top_n), top_names[::-1])
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'å‰{top_n}ä¸ªæœ€é‡è¦çš„ç‰¹å¾')
    plt.show()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {save_path}")


def save_predictions(model, X_test, y_test, y_pred, save_path):
    """
    ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœ

    Parameters:
    -----------
    model : RandomForestClassifier
        è®­ç»ƒå¥½çš„æ¨¡å‹
    X_test : array-like
        æµ‹è¯•ç‰¹å¾æ•°æ®
    y_test : array-like
        çœŸå®æ ‡ç­¾
    y_pred : array-like
        é¢„æµ‹æ ‡ç­¾
    save_path : Path or str
        ä¿å­˜è·¯å¾„
    """
    y_pred_proba = model.predict_proba(X_test)

    results = pd.DataFrame({
        'çœŸå®æ ‡ç­¾': y_test,
        'é¢„æµ‹æ ‡ç­¾': y_pred,
        'é¢„æµ‹æ­£ç¡®': y_test == y_pred,
        'æ— æ¯’æ¦‚ç‡': y_pred_proba[:, 0],
        'æœ‰æ¯’æ¦‚ç‡': y_pred_proba[:, 1]
    })

    # æ·»åŠ é¢„æµ‹ç»“æœè§£é‡Š
    results['çœŸå®ç±»åˆ«'] = results['çœŸå®æ ‡ç­¾'].map({0: 'æ— æ¯’', 1: 'æœ‰æ¯’'})
    results['é¢„æµ‹ç±»åˆ«'] = results['é¢„æµ‹æ ‡ç­¾'].map({0: 'æ— æ¯’', 1: 'æœ‰æ¯’'})

    results.to_csv(save_path, index=False)
    print(f"è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {save_path}")

    # æ˜¾ç¤ºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    errors = results[results['é¢„æµ‹æ­£ç¡®'] == False]
    if len(errors) > 0:
        print(f"\né”™è¯¯åˆ†ç±»çš„æ ·æœ¬æ•°: {len(errors)} ({len(errors) / len(y_test) * 100:.1f}%)")
        print("\né”™è¯¯åˆ†ç±»ç¤ºä¾‹:")
        print(errors[['çœŸå®ç±»åˆ«', 'é¢„æµ‹ç±»åˆ«', 'æ— æ¯’æ¦‚ç‡', 'æœ‰æ¯’æ¦‚ç‡']].head())


def load_and_process_data(train_file, test_files):
    """
    åŠ è½½å¹¶å¤„ç†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®

    Parameters:
    -----------
    train_file : Path
        è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
    test_files : list
        æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨

    Returns:
    --------
    X_train, y_train, X_test, y_test, feature_names : tuple
        å¤„ç†å¥½çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    """
    # 1. åŠ è½½å¹¶å¤„ç†è®­ç»ƒæ•°æ®
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_data = read_fasta(train_file)
    processed_train = process_data(train_data)

    # 2. å‡†å¤‡è®­ç»ƒç‰¹å¾å’Œæ ‡ç­¾
    X_train = processed_train.drop(['Sequence', 'toxicity'], axis=1)
    y_train = processed_train['toxicity'].astype(int)

    print(f"\nè®­ç»ƒé›†ä¿¡æ¯:")
    print(f"  ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
    print(f"  è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"    æ— æ¯’ (0): {(y_train == 0).sum()} æ ·æœ¬ ({((y_train == 0).sum() / len(y_train) * 100):.1f}%)")
    print(f"    æœ‰æ¯’ (1): {(y_train == 1).sum()} æ ·æœ¬ ({((y_train == 1).sum() / len(y_train) * 100):.1f}%)")

    # 3. åŠ è½½å¹¶å¤„ç†æµ‹è¯•æ•°æ®
    print("\nåŠ è½½æµ‹è¯•æ•°æ®...")
    test_data_list = []
    for test_file in test_files:
        test_data = read_fasta(test_file)
        test_data_list.append(test_data)

    test_data = pd.concat(test_data_list, ignore_index=True)
    processed_test = process_data(test_data)

    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(processed_test)}")

    # 4. å‡†å¤‡æµ‹è¯•ç‰¹å¾å’Œæ ‡ç­¾
    X_test = processed_test.drop(['Sequence', 'toxicity'], axis=1)
    y_test = processed_test['toxicity'].astype(int)

    # ç¡®ä¿æµ‹è¯•é›†çš„ç‰¹å¾åˆ—ä¸è®­ç»ƒé›†ä¸€è‡´
    missing_cols = set(X_train.columns) - set(X_test.columns)
    for col in missing_cols:
        X_test[col] = 0

    X_test = X_test[X_train.columns]  # ä¿æŒåˆ—é¡ºåºä¸€è‡´

    return X_train, y_train, X_test, y_test, X_train.columns.tolist()


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå’Œé¢„æµ‹æµç¨‹
    """
    # å®šä¹‰è·¯å¾„
    path = Path('/')
    train_file = path / 'data/train_data.fasta'
    test_files = [path / 'data/test1.fasta', path / 'data/test2.fasta']

    # åˆ›å»ºç»“æœç›®å½•
    results_dir = path / 'results'
    results_dir.mkdir(exist_ok=True)

    print("=" * 60)
    print("éšæœºæ£®æ—æ¯’æ€§é¢„æµ‹æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")
    print("=" * 60)

    # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
    X_train, y_train, X_test, y_test, feature_names = load_and_process_data(train_file, test_files)

    # 2. è®­ç»ƒæ¨¡å‹
    print("\nè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    rf_model, scaler = train_random_forest(X_train, y_train)

    # 3. æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®
    X_test_scaled = scaler.transform(X_test)

    # 4. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
    y_pred = rf_model.predict(X_test_scaled)
    y_pred_proba = rf_model.predict_proba(X_test_scaled)

    # 5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    correct_predictions = (y_test == y_pred).sum()
    total_predictions = len(y_test)
    correct_rate = correct_predictions / total_predictions

    print("\n" + "=" * 40)
    print("æ¨¡å‹è¯„ä¼°ç»“æœ:")
    print("=" * 40)
    print(f"  é¢„æµ‹æ­£ç¡®æ•°: {correct_predictions}/{total_predictions}")
    print(f"  é¢„æµ‹æ­£ç¡®ç‡: {correct_rate:.4f} ({correct_rate * 100:.2f}%)")
    print(f"  é¢„æµ‹é”™è¯¯æ•°: {total_predictions - correct_predictions}/{total_predictions}")
    print(f"  é¢„æµ‹é”™è¯¯ç‡: {1 - correct_rate:.4f} ({(1 - correct_rate) * 100:.2f}%)")
    print("-" * 40)
    print(f"  å‡†ç¡®ç‡ (Accuracy):  {accuracy:.4f}")
    print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"  å¬å›ç‡ (Recall):    {recall:.4f}")
    print(f"  F1åˆ†æ•° (F1-score):  {f1:.4f}")

    # 6. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_confusion_matrix(y_test, y_pred, results_dir / 'confusion_matrix.png')

    # 7. ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
    plot_feature_importance(rf_model, feature_names, results_dir / 'feature_importance.png')

    # 8. ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    save_predictions(rf_model, X_test_scaled, y_test, y_pred, results_dir / 'detailed_predictions.csv')

    # 9. ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶
    joblib.dump(rf_model, results_dir / 'random_forest_model.joblib')
    joblib.dump(scaler, results_dir / 'scaler.pkl')
    print(f"\næ¨¡å‹æ–‡ä»¶å·²ä¿å­˜è‡³: {results_dir / 'random_forest_model.joblib'}")
    print(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {results_dir / 'scaler.pkl'}")

    # 10. ç»“æœæ±‡æ€»
    print(f"\nğŸ“Š ç»“æœæ±‡æ€»:")
    print(f"  - è®­ç»ƒé›†å¤§å°: {len(X_train)} ä¸ªæ ·æœ¬")
    print(f"  - æµ‹è¯•é›†å¤§å°: {len(X_test)} ä¸ªæ ·æœ¬")
    print(f"  - ç‰¹å¾æ•°é‡: {X_train.shape[1]}")
    print(f"  - å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"  - F1åˆ†æ•°: {f1:.4f}")
    print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®: {results_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()