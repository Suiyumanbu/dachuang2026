import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, confusion_matrix, classification_report)
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import warnings
from machine_learning.process_data import process_data, read_fasta  # å¯¼å…¥å¤„ç†æ•°æ®çš„å‡½æ•°
import joblib

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams["font.sans-serif"] = ["SimHei"]
plt.rcParams["axes.unicode_minus"] = False

# å®šä¹‰è·¯å¾„
path = Path('/')

def prepare_features(df):
    """
    å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆæ ‡ç­¾å·²ç»æ˜¯'0'å’Œ'1'å­—ç¬¦ä¸²ï¼‰
    """
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop(['Sequence', 'toxicity'], axis=1)
    y = df['toxicity']

    # å°†å­—ç¬¦ä¸²'0'/'1'è½¬æ¢ä¸ºæ•´æ•°
    y_numeric = y.astype(int)

    return X, y_numeric

def train_random_forest_with_cv(X_train, y_train):
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    # ç®€å•çš„å‚æ•°ç½‘æ ¼ï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
    param_grid = {
        'n_estimators': [50, 100],
        'max_depth': [5, 10, None],
        'min_samples_split': [2, 5]
    }

    # ç½‘æ ¼æœç´¢ï¼ˆå†…éƒ¨ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯ï¼‰
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),
        param_grid,
        cv=5,
        scoring='f1',          # ä»¥F1åˆ†æ•°ä¸ºä¼˜åŒ–ç›®æ ‡
        n_jobs=-1
    )

    print("ğŸ” æ­£åœ¨è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼ˆç½‘æ ¼æœç´¢ï¼‰...")
    grid_search.fit(X_train_scaled, y_train)

    print(f"\nâœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯F1åˆ†æ•°: {grid_search.best_score_:.4f}")

    model = grid_search.best_estimator_

    return model, scaler

#%%
def evaluate_model(model, X_test, y_test):
    """
    è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
    """

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')  # äºŒåˆ†ç±»ç”¨binary
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')

    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"å¬å›ç‡ (Recall): {recall:.4f}")
    print(f"F1åˆ†æ•° (F1-score): {f1:.4f}")

    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=['æ— æ¯’(0)', 'æœ‰æ¯’(1)']))

    return y_pred, y_pred_proba
#%%
def plot_confusion_matrix(y_test, y_pred, save_path):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ
    """
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'],
                yticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'])
    plt.title('æ··æ·†çŸ©é˜µ - æµ‹è¯•é›†')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.show()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')

def plot_confusion_matrix(y_test, y_pred, save_path):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ
    """
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'],
                yticklabels=['æ— æ¯’(0)', 'æœ‰æ¯’(1)'])
    plt.title('æ··æ·†çŸ©é˜µ - æµ‹è¯•é›†')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.show()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')

def plot_feature_importance(model, feature_names, save_path, top_n=20):
    """
    ç»˜åˆ¶å¹¶ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾
    """
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    # å–å‰top_nä¸ªç‰¹å¾
    top_indices = indices[:top_n]
    top_importances = importances[top_indices]
    top_names = [feature_names[i] for i in top_indices]

    plt.figure(figsize=(10, 8))
    plt.barh(range(top_n), top_importances[::-1])
    plt.yticks(range(top_n), top_names[::-1])
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'å‰{top_n}ä¸ªæœ€é‡è¦çš„ç‰¹å¾')
    plt.show()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {save_path}")

def save_predictions(model, X_test, y_test, y_pred, save_path):
    """
    ä¿å­˜è¯¦ç»†çš„é¢„æµ‹ç»“æœ
    """
    y_pred_proba = model.predict_proba(X_test)

    results = pd.DataFrame({
        'çœŸå®æ ‡ç­¾': y_test,
        'é¢„æµ‹æ ‡ç­¾': y_pred,
        'é¢„æµ‹æ­£ç¡®': y_test == y_pred,
        'æ— æ¯’æ¦‚ç‡': y_pred_proba[:, 0],
        'æœ‰æ¯’æ¦‚ç‡': y_pred_proba[:, 1]
    })

    # æ·»åŠ é¢„æµ‹ç»“æœè§£é‡Š
    results['çœŸå®ç±»åˆ«'] = results['çœŸå®æ ‡ç­¾'].map({0: 'æ— æ¯’', 1: 'æœ‰æ¯’'})
    results['é¢„æµ‹ç±»åˆ«'] = results['é¢„æµ‹æ ‡ç­¾'].map({0: 'æ— æ¯’', 1: 'æœ‰æ¯’'})

    results.to_csv(save_path, index=False)
    print(f"è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {save_path}")

    # æ˜¾ç¤ºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    errors = results[results['é¢„æµ‹æ­£ç¡®'] == False]
    if len(errors) > 0:
        print(f"\né”™è¯¯åˆ†ç±»çš„æ ·æœ¬æ•°: {len(errors)} ({len(errors)/len(y_test)*100:.1f}%)")
        print("\né”™è¯¯åˆ†ç±»ç¤ºä¾‹:")
        print(errors[['çœŸå®ç±»åˆ«', 'é¢„æµ‹ç±»åˆ«', 'æ— æ¯’æ¦‚ç‡', 'æœ‰æ¯’æ¦‚ç‡']].head())

def main():
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = path / 'results1'
    results_dir.mkdir(exist_ok=True)

    #  1. åŠ è½½å¹¶å¤„ç†è®­ç»ƒæ•°æ®
    train_file = path / 'data/train_data.fasta'
    train_data = read_fasta(train_file)
    processed_train = process_data(train_data)

    #  2. å‡†å¤‡è®­ç»ƒç‰¹å¾å’Œæ ‡ç­¾
    X_train = processed_train.drop(['Sequence', 'toxicity'], axis=1)
    y_train = processed_train['toxicity'].astype(int)  # å­—ç¬¦ä¸²'0'/'1'è½¬æ•´æ•°
    # æ˜¾ç¤ºè®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ
    print(f"  ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
    print(f"  è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"    æ— æ¯’ (0): {(y_train == 0).sum()} æ ·æœ¬ ({((y_train == 0).sum() / len(y_train) * 100):.1f}%)")
    print(f"    æœ‰æ¯’ (1): {(y_train == 1).sum()} æ ·æœ¬ ({((y_train == 1).sum() / len(y_train) * 100):.1f}%)")

    #  3. è®­ç»ƒæ¨¡å‹
    rf_model, scaler = train_random_forest_with_cv(X_train, y_train)

    #  4. åŠ è½½å¹¶å¤„ç†æµ‹è¯•æ•°æ®
    test_file1 = path / 'data/test1.fasta'
    test_data1 = read_fasta(test_file1)
    test_file2 = path / 'data/test2.fasta'
    test_data2 = read_fasta(test_file2)
    test_data = pd.concat([test_data1, test_data2], ignore_index=True)
    processed_test = process_data(test_data)
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(processed_test)}")

    #  5. å‡†å¤‡æµ‹è¯•ç‰¹å¾å’Œæ ‡ç­¾
    X_test = processed_test.drop(['Sequence', 'toxicity'], axis=1)
    y_test = processed_test['toxicity'].astype(int)
    # ç¡®ä¿æµ‹è¯•é›†çš„ç‰¹å¾åˆ—ä¸è®­ç»ƒé›†ä¸€è‡´
    missing_cols = set(X_train.columns) - set(X_test.columns)
    for col in missing_cols:
        X_test[col] = 0

    X_test = X_test[X_train.columns]  # ä¿æŒåˆ—é¡ºåºä¸€è‡´
    # æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®
    X_test_scaled = scaler.transform(X_test)

    #  6. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
    y_pred = rf_model.predict(X_test_scaled)
    y_pred_proba = rf_model.predict_proba(X_test_scaled)

    #  7. æ¨¡å‹è¯„ä¼°ç»“æœ
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    # è®¡ç®—é¢„æµ‹æ­£ç¡®ç‡
    correct_predictions = (y_test == y_pred).sum()
    total_predictions = len(y_test)
    correct_rate = correct_predictions / total_predictions

    print(f"  é¢„æµ‹æ­£ç¡®æ•°: {correct_predictions}/{total_predictions}")
    print(f"  é¢„æµ‹æ­£ç¡®ç‡: {correct_rate:.4f} ({correct_rate * 100:.2f}%)")
    print(f"  é¢„æµ‹é”™è¯¯æ•°: {total_predictions - correct_predictions}/{total_predictions}")
    print(f"  é¢„æµ‹é”™è¯¯ç‡: {1 - correct_rate:.4f} ({(1 - correct_rate) * 100:.2f}%)")
    print("-" * 40)
    print(f"  å‡†ç¡®ç‡ (Accuracy):  {accuracy:.4f}")
    print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"  å¬å›ç‡ (Recall):    {recall:.4f}")
    print(f"  F1åˆ†æ•° (F1-score):  {f1:.4f}")

    #  8. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(y_test, y_pred, results_dir / 'confusion_matrix.png')
    plot_feature_importance(rf_model, X_train.columns, results_dir / 'feature_importance.png')

    #  9. ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    save_predictions(rf_model, X_test_scaled, y_test, y_pred, results_dir / 'detailed_predictions.csv')

    #  10. ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶
    joblib.dump(rf_model, results_dir / 'random_forest_model.joblib')
    joblib.dump(scaler, results_dir / 'scaler.pkl')

    #  11. ç»“æœæ±‡æ€»
    print(f"\nğŸ“Š ç»“æœæ±‡æ€»:")
    print(f"  - è®­ç»ƒé›†å¤§å°: {len(X_train)} ä¸ªæ ·æœ¬")
    print(f"  - æµ‹è¯•é›†å¤§å°: {len(X_test)} ä¸ªæ ·æœ¬")
    print(f"  - ç‰¹å¾æ•°é‡: {X_train.shape[1]}")
    print(f"  - å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"  - F1åˆ†æ•°: {f1:.4f}")
    print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®: {results_dir}")

if __name__ == "__main__":
    main()